---
title: "Presentation"
editor: visual
author: "Justyn Rodrigues"
suppress-bibliography: false
format:
  revealjs:
    theme: simple # sky simple beige blood if I change the mermaid line colour
    bibliography: references.bib
    smaller: true
    scrollable: true
    mermaid: 
      theme: forest
slide-number: true
---

# The XAI Knight Rises: Unmasking Biases in Face Recognition

## Framing the Problem

```{r message=FALSE, include=FALSE}
library(gt)
```

------------------------------------------------------------------------

### Introduction

-   Sentence about AI popularity rising ðŸ“ˆ, increasing its usage ðŸš€, thus a higher emphasis on needing to understand how it works
-   Particularly, we want to address
    -   how do we know we can trust AI?
    -   how do we know if the model is doing what we want?
    -   how can we evaluate if the model is fair?

------------------------------------------------------------------------

### What is XAI?

-   To address this, eXplainable Artificial Intelligence (XAI) has become a key research focus.
-   The general idea is that XAI should ideally help humans understand or see into the blackbox of AI
-   While it may sound simple there are many key challenges with XAI
    -   what is an explanation?
    -   is XAI helpful?
    -   how do we know we can trust XAI?
    -   its name by definition is vague and creates confusion
    -   what type of XAI is best?

## Types of XAI I

```{mermaid}
%%| fig-height: 6
flowchart LR
    A(XAI Methods)
    B(Post-hoc Explanations)
    C(Transparency)
    C1(Algorithmic Transparency)
    C2(Decomposability)
    C3(Simulatability)
    A1(Backpropagation-based)
    A2(Perturbation-based)
    A3A(Global)
    A3B(Local)

    A --> B & C
    C --> C1 & C2 & C3
    B --> A3A & A3B
    subgraph We are here
    A3B --> A1 & A2
    end
```

## Types of XAI I

In short, we can understand the types of XAI as

-   Transparency: understanding the methodological aspects
-   Post-hoc Explanations: have a model show you what it believes is important for its decision
    -   global: the entire model
    -   local: single instance

## XAI Challenges and Limitations

-   How long to provide context, etc. for XAI prior covering the user study

## Misc

### Misc I

XAI Characteristics

-   When [@lipton2018mythos]
    -   Post-hoc: get explanation from the outcome (what part of input is responsible)
-   Model type
-   Explanation level [@molnar2022]
    -   model specific: specific to a models architecture
    -   model agnostic: can be used on any model
    -   global: the entire model
        -   Sheds light on the big picture biases affecting larger subgroups
        -   Is the model at a high level ready for deployment?
    -   local: single prediction
        -   Uncover biases in the local neighbourhood of a given instance
        -   Are predictions being made for the right reasons
-   Visualisation type
    -   Occlusion or perturbation-based: Methods like SHAP and LIME [@ribeiro2016should] cover a small area of the image with a black or grey square, then calculate the impact this has on the model prediction.
    -   Saliency maps: Highlight the area of the input that is most relevant for the model output.
        -   Gradient-based (Sensitivity) [@konate2021comparison]
        -   Signal methods [@kindermans2019reliability]
        -   Attribution methods
    -   Prototypes

------------------------------------------------------------------------

### Misc II

Still need to cover limitations so I can show why my study is needed lol.

-   Faithfulness
    -   Do explanations change when prediction changes? e.g. [@adebayo2018sanity]
    -   some explanation methods do not 'reflect' the underlying model.
-   Stability
    -   slight changes to inputs can cause large changes in explanations.
    -   explanations can be sensetive to hyperparameters [@bansal2020sam]
-   Fragility
    -   Do explanations change under distribution shift? e.g. [@kindermans2019reliability]
    -   post-hoc explanations can be easily manipulated.
    -   explanations can be easy to manipulate, e.g. [@dombrowski2019explanations], however, the prediction did not change
-   Evaluation
    -   It is tough to assess when there is no groundtruth.
    -   The evaluation is mostly qualitative.
    -   If we knew what was important to the model then we would not need an explanation.
    -   Are explanations better than random guess? [@hooker2019benchmark]
        -   popular explainability methods were no better than random designation of feature importance

------------------------------------------------------------------------

### Misc III

Post-hoc XAI are fragile.

-   many gradient based XAI are instable
-   perturbation methods can be easily manipulated (check report for citation?)
-   explanations can be easy to manipulate, e.g. [@dombrowski2019explanations], however, the prediction did not change
-   [@kindermans2019reliability] showed that attribution methods result in limited reliability for input variation.
-   [@ghorbani2019interpretation] showed that introducing small (adversarial) perturbations to an image, can result in the same model prediction while showing different pixels being highlighted as explanations.

------------------------------------------------------------------------

### Misc IV

-   Unfortunately, the human tendency is to ascribe a positive interpretation: we assume that the feature we would find important is the one that was used (this is an example of a famously harmful cognitive error called confirmation bias).
-   Moreover, [@Gu2019NeuralNM] showed that common visual explanations remain unchanged even when precise modifications are made to the input that substantially alter the model's predictions (a process known as an adversarial attack), even when those attacks lead to incorrect model predictions.
-   As with heat maps, the human tendency is to assume that a model has used words in the same way we would.
-   All of these examples reveal another major challenge: explanations have no performance guarantees.
-   As such, using post-hoc explanations to assess the quality of model decisions adds an additional source of error---not only can the model be right or wrong, but so can the explanation.
-   An explanation does not show how the model arrived at the decision. For example, in CV it will show you the area that it thinks is important for the decision, but how do we assess if it was reasonable for it to be looking in that specific location?

## Rough Outline

-   Introduction
-   Background on XAI
-   Methodology
-   Results and Discussion
-   Conclusion

## Random Stuff

::: columns
::: {.column width="40%"}
-   Faithfulness
-   Stability
-   Fragility
-   [Evaluation]{style="font-weight:bold;background-color:#ccddeb;"}
:::

::: {.column width="60%"}
::: {.fragment fragment-index="2"}
::: in-out
-   It is tough to assess when there is no groundtruth.
-   The evaluation is mostly qualitative.\
-   If we knew what was important to the model then we would not need an explanation.
:::
:::

::: {.fragment fragment-index="3"}
::: in-out
-   Are explanations better than random guess? [@hooker2019benchmark]
    -   popular explainability methods were no better than random designation of feature importance
:::
:::
:::
:::

## Study

-   User study was ran on Prolific [^1] to explore if post-hoc explanations help humans recognise when a face recognition model is biased.
-   The Diverse Human Faces dataset by Synthesis AI [@diversehumanfacesdataset] was used over real faces to ensure labels were correct for features such as skin tone, yaw direction, lighting, among other things
-   Comment that there is not a significant gap between face recognition using synthetic data and real faces
-   Comment how you tried 3DMM?

[^1]: https://www.prolific.co/

------------------------------------------------------------------------

## Processing of Data

### Skin Tone

-   To categorise skin tone, the Fitzpatrick classification system was used [@fitzpatrick1988validity], where the middle group was removed to make it slightly more clear.

```{r}
skin_tone_labels <- data.frame(
  Label = c('Light Skin Tones', 'Dark Skin Tones', 'Misc'),
  `Rounded Values` = c('1, 2', '4, 5, 6', '3'), 
  n = c(3900, 2900, 3200),
  Prop = c(0.39, 0.29, 0.32)
)

skin_tone_labels |>
  gt() |>
  cols_label(Rounded.Values = 'Rounded Values') |>
  tab_header(title = 'Skin Tone Labels') |>
  cols_width(
    n ~ px(50),
    Prop ~ px(50),
    everything() ~ px(150)) |>
  cols_align(columns = c(Rounded.Values, n, Prop), align = 'left')
```

------------------------------------------------------------------------

### Yaw Position

-   Similarly, yaw directions between middle and side were removed.

```{r}
yaw_labels <- data.frame(
  Label = c('Middle', 'Side', 'Vacant'), 
  `Yaw Range(s)` = c('-4Ëš to 4Ëš', '-15Ëš to -7Ëš, 7Ëš to 15Ëš', '-7Ëš to -4Ëš, 4Ëš to 7Ëš'), 
  n = c(2650, 5334, 2016), 
  Prop = c(0.265, 0.533, 0.202)
)

yaw_labels |>
  gt() |>
  tab_source_note(source_note = "Note: Side and vacant yaw ranges include two different ranges.") |>
  cols_label(Yaw.Range.s. = 'Yaw Range(s)') |>
  tab_header(title = 'Yaw Direction Labels') |>
  cols_width(Yaw.Range.s. ~ px(150))
```

::: {layout-ncol="2" style="display: flex; justify-content: center;"}
![Middle Yaw](images/6899.cam_default.f_1.rgb.png){width="50%"}

![Side Yaw](images/6805.cam_default.f_1.rgb.png){width="50%"}
:::

------------------------------------------------------------------------

### Creating Groups

-   Then using skin tone and yaw position, three models were trained using a different proportion of labels.
-   The two undersampled groups had the number for both skin labels balanced, so the difference could not be attributed simply due to sample size difference
-   For all conditions the sex of the rendered human id was balanced

```{r}
data.frame(
  Model = c('Fair', 'Dark Undersampled', 'Light Undersampled'),
  yaw_dark = c('All', 'Middle', 'All'),
  yaw_light = c('All', 'All', 'Middle')
) |>
  gt() |>
  tab_spanner(label = 'Yaw Position', columns = c(yaw_dark, yaw_light)) |>
  cols_label(yaw_dark = 'Dark Skin Tones', yaw_light = 'Light Skin Tones') |>
  tab_header(title = 'Experiment Models') |>
  cols_width(
    Model ~ px(170),
    starts_with('yaw') ~ px(150)
  )
```

------------------------------------------------------------------------

### hi

-   Now that we have these three models, we will provide a user with an input image, the predicted person, then an explanation from two difference models, asking the user to select which XAI they find to be more reasonable.
-   Every participant will experience the condition made up of the Light and Dark undersampled groups, with the second condition they experience being randomly assigned.
-   The order of the trials within a block is randomised, and the position of the model is counterbalanced, e.g. for the model that everybody experiences, half will see the Dark Undersampled model as `Model 1`, any the other half will see it as `Model 2`. Likewise, the order of the blocks is counterbalanced, e.g. which condition they experience first.

```{r}
exp_cond <- data.frame(
  #Block = c('A', 'B', 'C'),
  condition1 = c('Light', 'Fair', 'Fair'),
  condition2 = c('Dark', 'Dark', 'Light'),
  factor_type = c('Between-subjects', 'Within-subjects', 'Within-subjects'),
  condition_name = c('Biased', 'Fair-Dark', 'Fair-Light')
)

exp_cond |>
  gt() |>
  tab_header(title = 'Experiment Conditions') |>
  tab_footnote(footnote = 'Note: each block is counterbalanced.') |>
  tab_footnote(footnote = 'Light and Dark refer to the undersampled group.', locations = cells_column_labels(columns = c(condition1, condition2))) |>
  cols_label(condition1 = 'Condition 1', condition2 = 'Condition 2', factor_type = 'Factor Type', condition_name = 'Condition Name') |>
  cols_width(
    starts_with('condition') ~ px(125),
    factor_type ~ px(175),
    condition_name ~ px(180)
    )
```

## Results

### Participants

-   Made the study available to participants from either Australia, United States, or United Kingdom. (mention why?)
-   The original sample was 50, but used 10 for a pilot prior running the study.
-   Staggered out the study to try and get a balance between the countries and types of people.
-   Countries:
    -   Australia 7
    -   United Kingdom 23
    -   United States 10

```{r}
part_demo <- data.frame(
  age_years = c('18 - 30', '30 - 45', '45 - 65', '65 +', 'All'),
  female = c('9 (39.1)', '7 (63.6)', '0 (0.0)', '1 (50.0)', '17 (42.5)'),
  male = c('14 (60.9)', '4 (36.4)', '4 (100.0)', '1 (50.0)', '23 (57.5)'),
  all = c('23 (100.0)', '11 (100.0)', '4 (100.0)', '2 (100.0)', '40 (100.0)')
)

part_demo |>
  gt() |>
  tab_spanner(label = 'Sex, n (%)', columns = c(female, male, all)) |>
  cols_label(age_years = 'Age (Years)', female = 'Female', male = 'Male', all = 'All') |>
  tab_header(title = 'Participant Demographics') |>
  cols_width(everything() ~ px(105))
```

------------------------------------------------------------------------

### Selected Model

-   The Dark Undersampled model was the most selected model as the most reasonable
-   Likewise, the balanced model was selected the least frequently as the most reasonable model

```{r}
selected_model <- data.frame(
  stat = c('Observed', 'Expected', 'Std. Residuals'),
  dark_und = c('481 (0.391)', '410 (0.33)', '4.29'),
  light_und = c('405 (0.329)', '410 (0.33)', '- .302'),
  fair = c('344 (0.280)', '410 (0.33)', '- 3.99')
)

selected_model |>
  gt(rowname_col = 'stat') |>
  cols_label(dark_und = 'Dark Undersampled', light_und = 'Light Undersampled', fair = 'Fair') |>
  cols_align(align = 'left') |>
  tab_header(title = 'Contingency Table of Selected Model') |>
  tab_footnote(footnote = 'Note: Count, (%)') |>
  cols_width(
    ends_with('_und') ~ px(160),
    fair ~ px(125)
  )
```

------------------------------------------------------------------------

### Posthoc

```{r}
post_hoc <- data.frame(
  Comparison = c('Dark Undersampled vs Light Undersampled', 'Dark Undersampled vs Fair', 'Light Undersampled vs Fair'),
  x2 = c(6.52, 22.8, 4.97),
  adusted_p = c('.0214', '< .0001', '.0258')
)

post_hoc |>
  gt() |>
  tab_header(title = 'Pairwise Post Hoc Chi-Square Tests') |>
  tab_footnote(footnote = 'Adjusted using Holm-Bonferroni method.', locations = cells_column_labels(columns = adusted_p)) |>
  cols_label(x2 = html('&#120536; <sup>2</sup>'), adusted_p = 'Adjusted p') |>
  cols_width(
    Comparison ~ px(315),
    everything() ~ px(100)
  ) |>
  cols_align(align = 'left')
```

------------------------------------------------------------------------

### Example 1

![Example Trial](images/xai_study_example.png){width="80%"}

------------------------------------------------------------------------

### Selected Model by Confidence

:::
![Selected Model by Confidence](images/selected_model_by_confidence.png){width="90%"; fig-align="center"}
:::

------------------------------------------------------------------------

## References
