@article{lipton2018mythos,
	title        = {The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.},
	author       = {Lipton, Zachary C},
	year         = 2018,
	journal      = {Queue},
	publisher    = {ACM New York, NY, USA},
	volume       = 16,
	number       = 3,
	pages        = {31--57}
}
@book{molnar2022,
	title        = {Interpretable Machine Learning},
	author       = {Christoph Molnar},
	year         = 2022,
	url          = {https://christophm.github.io/interpretable-ml-book},
	subtitle     = {A Guide for Making Black Box Models Explainable},
	edition      = 2
}
@inproceedings{konate2021comparison,
	title        = {A Comparison of Saliency Methods for Deep Learning Explainability},
	author       = {Konate, Salamata and Lebrat, L{\'e}o and Santa Cruz, Rodrigo and Smith, Elliot and Bradley, Andrew and Fookes, Clinton and Salvado, Olivier},
	year         = 2021,
	booktitle    = {2021 Digital Image Computing: Techniques and Applications (DICTA)},
	pages        = {01--08},
	organization = {IEEE}
}
@incollection{kindermans2019reliability,
	title        = {The (un) reliability of saliency methods},
	author       = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"u}tt, Kristof T and D{\"a}hne, Sven and Erhan, Dumitru and Kim, Been},
	year         = 2019,
	booktitle    = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
	publisher    = {Springer},
	pages        = {267--280}
}
@inproceedings{ribeiro2016should,
	title        = {" Why should i trust you?" Explaining the predictions of any classifier},
	author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year         = 2016,
	booktitle    = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
	pages        = {1135--1144}
}
@article{hooker2019benchmark,
	title        = {A benchmark for interpretability methods in deep neural networks},
	author       = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
	year         = 2019,
	journal      = {Advances in neural information processing systems},
	volume       = 32
}
@article{dombrowski2019explanations,
	title        = {Explanations can be manipulated and geometry is to blame},
	author       = {Dombrowski, Ann-Kathrin and Alber, Maximillian and Anders, Christopher and Ackermann, Marcel and M{\"u}ller, Klaus-Robert and Kessel, Pan},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@inproceedings{bansal2020sam,
	title        = {Sam: The sensitivity of attribution methods to hyperparameters},
	author       = {Bansal, Naman and Agarwal, Chirag and Nguyen, Anh},
	year         = 2020,
	booktitle    = {Proceedings of the ieee/cvf conference on computer vision and pattern recognition},
	pages        = {8673--8683}
}
@article{adebayo2018sanity,
	title        = {Sanity checks for saliency maps},
	author       = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	year         = 2018,
	journal      = {Advances in neural information processing systems},
	volume       = 31
}
@inproceedings{ghorbani2019interpretation,
	title        = {Interpretation of neural networks is fragile},
	author       = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
	year         = 2019,
	booktitle    = {Proceedings of the AAAI conference on artificial intelligence},
	volume       = 33,
	number       = {01},
	pages        = {3681--3688}
}
@article{Gu2019NeuralNM,
  title={Neural Network Memorization Dissection},
  author={Jindong Gu and Volker Tresp},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.09537}
}