---
title: "The XAI Knight Rises: Unmasking Biases in Face Recognition"
author: "Justyn Rodrigues"
suppress-bibliography: false
editor: visual
format:
  revealjs:
    theme: simple # sky simple beige blood if I change the mermaid line colour
    bibliography: references.bib
    smaller: true
    scrollable: true
    mermaid: 
      theme: forest
slide-number: c # true
---

```{r library, message=FALSE, include=FALSE}
library(gt)
```

## Introduction

To do list

- issues with ambiguity/terms used so you can use the hotdog example
- challenge of subjective vs objective measures in user studies?
- user study count?
- misc notes
   - Occlusion or perturbation-based: Methods like SHAP and LIME [@ribeiro2016should] cover a small area of the image with a black or grey square, then calculate the impact this has on the model prediction.
   - Saliency maps: Highlight the area of the input that is most relevant for the model output.
- sort through the challenges 
- Processing of Data (how labels/models were created)
- Training the model (mention it was pretrained)
- User study experiment, e.g. task, conditions
- how we are evaluating it?
- specific techniques used? e.g. LRP?

---

### What is the problem?

-   Sentence about AI popularity rising ðŸ“ˆ, increasing its usage ðŸš€, thus a higher emphasis on needing to understand how it works
-   Particularly, we want to address
    -   how do we know we can trust AI?
    -   how do we know if the model is doing what we want?
    -   how can we evaluate if the model is fair?

---

### What is XAI?

-   To address this, eXplainable Artificial Intelligence (XAI) has become a key research focus.
-   The general idea is that XAI should ideally help humans understand or see into the blackbox of AI
-   While it may sound simple there are many key challenges with XAI
    -   what is an explanation?
    -   is interpretability the same as explainability?
    -   can we trust explanations?
    -   are explanations helpful?
    -   what type of XAI is best?

::: aside
its name by definition is vague and creates confusion
:::

---

### Types of XAI

```{mermaid}
%%| fig-height: 6
flowchart LR
    A(XAI Methods)
    B(Post-hoc Explanations)
    C(Transparency)
    C1(Algorithmic Transparency)
    C2(Decomposability)
    C3(Simulatability)
    A1(Backpropagation-based)
    A2(Perturbation-based)
    A3A(Global)
    A3B(Local)

    A --> B & C
    C --> C1 & C2 & C3
    B --> A3A & A3B
    subgraph We are here
    A3B --> A1 & A2
    end
```

::: {.notes}
Transparency: understanding the methodological aspects.  
Post-hoc Explanations: have a model show you what it believes is important for its decision.  
global: the entire model; Sheds light on the big picture biases affecting larger subgroups
local: single prediction; Are predictions being made for the right reasons
:::

---

### XAI Challenges I

- A key challenge is that terms such as interpretable, explainable, transparent, etc. have no consensus. Specifically, some authors will use these terms interchangeably [@molnar2022; @lipton2018mythos], where other authors will use different meanings for these [@Rudin2018StopEB]. 
- Even when authors explicitly state the definition they are referring to, this is still an issue. To illustrate, for the rest of this presentation when I talk about a sandwich, I am referring to a hotdog.

---

### XAI Challenges II

- this is to show why the study is needed
- perturbation methods can be easily manipulated (check report for citation?)
- explanations can be easy to manipulate, e.g. [@dombrowski2019explanations], however, the prediction did not change
- [@ghorbani2019interpretation] showed that introducing small (adversarial) perturbations to an image, can result in the same model prediction while showing different pixels being highlighted as explanations.

---

### XAI Challenges III

- this can be more focused on the human side 
- include the number of user studies, and also the point about ambiguity of definitions
-   Unfortunately, the human tendency is to ascribe a positive interpretation: we assume that the feature we would find important is the one that was used (this is an example of a famously harmful cognitive error called confirmation bias).
-   Moreover, [@Gu2019NeuralNM] showed that common visual explanations remain unchanged even when precise modifications are made to the input that substantially alter the model's predictions (a process known as an adversarial attack), even when those attacks lead to incorrect model predictions.
-   As with heat maps, the human tendency is to assume that a model has used words in the same way we would.
-   All of these examples reveal another major challenge: explanations have no performance guarantees.
-   As such, using post-hoc explanations to assess the quality of model decisions adds an additional source of error---not only can the model be right or wrong, but so can the explanation.

---

### XAI Challenges IV

-   Do explanations change when prediction changes? e.g. [@adebayo2018sanity]
    -   some explanation methods do not 'reflect' the underlying model.
-   slight changes to inputs can cause large changes in explanations.
    -   explanations can be sensetive to hyperparameters [@bansal2020sam]
-   Do explanations change under distribution shift? e.g. [@kindermans2019reliability]
    -   post-hoc explanations can be easily manipulated.
    -   explanations can be easy to manipulate, e.g. [@dombrowski2019explanations], however, the prediction did not change
-   How can we evaluate XAI?
    -   It is tough to assess when there is no groundtruth.
    -   The evaluation is mostly qualitative.
    -   If we knew what was important to the model then we would not need an explanation.
    
::: aside
this might go earlier? or adjust before when you show some XAI questions
:::

---

### XAI Challenges V: User Studies

- Despite the attention that XAI has received, there still remains a signifcant limitation in the literature: user studies. 
- [-@adadi2018peeking] reported that only 5% of XAI publications are user studies.
- While this is low, it has been estimated to be closer to 1% [@keane2019case], with many of the available user studies on XAI not reporting statistical analysis.
- maybe mention that this might be because it is a hard problem?

---

## Methodology

### Introduce the study
### Tasks and Materials

- task?

#### Skin Tone

-   To categorise skin tone, the Fitzpatrick classification system was used [@fitzpatrick1988validity], where the middle group was removed to make it slightly more clear.

```{r}
skin_tone_labels <- data.frame(
  Label = c('Light Skin Tones', 'Dark Skin Tones', 'Misc'),
  `Rounded Values` = c('1, 2', '4, 5, 6', '3'), 
  n = c(3900, 2900, 3200),
  Prop = c(0.39, 0.29, 0.32)
)

skin_tone_labels |>
  gt() |>
  cols_label(Rounded.Values = 'Rounded Values') |>
  tab_header(title = 'Skin Tone Labels') |>
  cols_width(
    n ~ px(50),
    Prop ~ px(50),
    everything() ~ px(150)) |>
  cols_align(columns = c(Rounded.Values, n, Prop), align = 'left')
```

---

#### Yaw Position

-   Similarly, yaw directions between middle and side were removed.

```{r}
yaw_labels <- data.frame(
  Label = c('Middle', 'Side', 'Vacant'), 
  `Yaw Range(s)` = c('-4Ëš to 4Ëš', '-15Ëš to -7Ëš, 7Ëš to 15Ëš', '-7Ëš to -4Ëš, 4Ëš to 7Ëš'), 
  n = c(2650, 5334, 2016), 
  Prop = c(0.265, 0.533, 0.202)
)

yaw_labels |>
  gt() |>
  tab_source_note(source_note = "Note: Side and vacant yaw ranges include two different ranges.") |>
  cols_label(Yaw.Range.s. = 'Yaw Range(s)') |>
  tab_header(title = 'Yaw Direction Labels') |>
  cols_width(Yaw.Range.s. ~ px(150))
```

::: {layout-ncol="2" style="display: flex; justify-content: center;"}
![Middle Yaw](images/6899.cam_default.f_1.rgb.png){width="50%"}

![Side Yaw](images/6805.cam_default.f_1.rgb.png){width="50%"}
:::

---

#### Creating Groups

-   Then using skin tone and yaw position, three models were trained using a different proportion of labels.
-   The two undersampled groups had the number for both skin labels balanced, so the difference could not be attributed simply due to sample size difference
-   For all conditions the sex of the rendered human id was balanced

```{r}
data.frame(
  Model = c('Fair', 'Dark Undersampled', 'Light Undersampled'),
  yaw_dark = c('All', 'Middle', 'All'),
  yaw_light = c('All', 'All', 'Middle')
) |>
  gt() |>
  tab_spanner(label = 'Yaw Position', columns = c(yaw_dark, yaw_light)) |>
  cols_label(yaw_dark = 'Dark Skin Tones', yaw_light = 'Light Skin Tones') |>
  tab_header(title = 'Experiment Models') |>
  cols_width(
    Model ~ px(170),
    starts_with('yaw') ~ px(150)
  )
```

---

#### Experiment Conditions

-   Now that we have these three models, we will provide a user with an input image, the predicted person, then an explanation from two difference models, asking the user to select which XAI they find to be more reasonable.
-   Every participant will experience the condition made up of the Light and Dark undersampled groups, with the second condition they experience being randomly assigned.
-   The order of the trials within a block is randomised, and the position of the model is counterbalanced, e.g. for the model that everybody experiences, half will see the Dark Undersampled model as `Model 1`, any the other half will see it as `Model 2`. Likewise, the order of the blocks is counterbalanced, e.g. which condition they experience first.

```{r}
exp_cond <- data.frame(
  #Block = c('A', 'B', 'C'),
  condition1 = c('Light', 'Fair', 'Fair'),
  condition2 = c('Dark', 'Dark', 'Light'),
  factor_type = c('Between-subjects', 'Within-subjects', 'Within-subjects'),
  condition_name = c('Biased', 'Fair-Dark', 'Fair-Light')
)

exp_cond |>
  gt() |>
  tab_header(title = 'Experiment Conditions') |>
  tab_footnote(footnote = 'Note: each block is counterbalanced.') |>
  tab_footnote(footnote = 'Light and Dark refer to the undersampled group.', locations = cells_column_labels(columns = c(condition1, condition2))) |>
  cols_label(condition1 = 'Condition 1', condition2 = 'Condition 2', factor_type = 'Factor Type', condition_name = 'Condition Name') |>
  cols_width(
    starts_with('condition') ~ px(125),
    factor_type ~ px(175),
    condition_name ~ px(180)
    )
```

---

### Participant Info

-   Made the study available to participants from either Australia, United States, or United Kingdom. (mention why?)
-   The original sample was 50, but used 10 for a pilot prior running the study.
-   Staggered out the study to try and get a balance between the countries and types of people.
-   Countries:
    -   Australia 7
    -   United Kingdom 23
    -   United States 10

```{r}
part_demo <- data.frame(
  age_years = c('18 - 30', '30 - 45', '45 - 65', '65 +', 'All'),
  female = c('9 (39.1)', '7 (63.6)', '0 (0.0)', '1 (50.0)', '17 (42.5)'),
  male = c('14 (60.9)', '4 (36.4)', '4 (100.0)', '1 (50.0)', '23 (57.5)'),
  all = c('23 (100.0)', '11 (100.0)', '4 (100.0)', '2 (100.0)', '40 (100.0)')
)

part_demo |>
  gt() |>
  tab_spanner(label = 'Sex, n (%)', columns = c(female, male, all)) |>
  cols_label(age_years = 'Age (Years)', female = 'Female', male = 'Male', all = 'All') |>
  tab_header(title = 'Participant Demographics') |>
  cols_width(everything() ~ px(105))
```

---

## Results/Discussion

### Selected Model

-   The Dark Undersampled model was the most selected model as the most reasonable
-   Likewise, the balanced model was selected the least frequently as the most reasonable model

```{r}
selected_model <- data.frame(
  stat = c('Observed', 'Expected', 'Std. Residuals'),
  dark_und = c('481 (0.391)', '410 (0.33)', '4.29'),
  light_und = c('405 (0.329)', '410 (0.33)', '- .302'),
  fair = c('344 (0.280)', '410 (0.33)', '- 3.99')
)

selected_model |>
  gt(rowname_col = 'stat') |>
  cols_label(dark_und = 'Dark Undersampled', light_und = 'Light Undersampled', fair = 'Fair') |>
  cols_align(align = 'left') |>
  tab_header(title = 'Contingency Table of Selected Model') |>
  tab_footnote(footnote = 'Note: Count, (%)') |>
  cols_width(
    ends_with('_und') ~ px(160),
    fair ~ px(125)
  )
```


---

## Conclusion

### visualise sandwich, then ask what sauce they saw on their hotdog? 