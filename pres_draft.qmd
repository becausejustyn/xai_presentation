---
title: "The XAI Knight Rises: Unmasking Biases in Face Recognition"
author: "Justyn Rodrigues"
suppress-bibliography: false
editor: visual
format:
  revealjs:
    theme: simple # sky simple beige blood if I change the mermaid line colour
    bibliography: references.bib
    smaller: true
    scrollable: true
    mermaid: 
      theme: forest
slide-number: c # true
---

```{r library, message=FALSE, include=FALSE}
library(gt)
```

## What is the problem?

-   Artifical intelligence (AI) can be compared to a blackbox, in that we do not really understand what is happening under the hood.
-   As AI continues to grow in popularity, the need to understand how decisions are made increase.
-   Particularly, we want to address
    -   how do we know we can trust AI?
    -   how do we know if the model is doing what we want?
    -   how can we evaluate if the model is fair?

------------------------------------------------------------------------

## What is XAI?

-   To address this, eXplainable Artificial Intelligence (XAI) has become a key research focus.
-   The general idea is that XAI should ideally help humans understand or see into the blackbox of AI.
-   While it may sound simple there are many key challenges with XAI:
    -   what is an explanation?
    -   is interpretability the same as explainability?
    -   can we trust explanations?
    -   are explanations helpful?
    -   what type of XAI is best?

------------------------------------------------------------------------

## Types of XAI

```{mermaid}
%%| fig-height: 6
flowchart LR
    A(XAI Methods)
    B(Post-hoc Explanations)
    C(Transparency)
    C1(Algorithmic Transparency)
    C2(Decomposability)
    C3(Simulatability)
    A1(Backpropagation-based)
    A2(Perturbation-based)
    A3A(Global)
    A3B(Local)

    A --> B & C
    C --> C1 & C2 & C3
    B --> A3A & A3B
    subgraph Scope for today
    A3B --> A1 & A2
    end
```

::: notes
Transparency: understanding the methodological aspects.\
Post-hoc Explanations: have a model show you what it believes is important for its decision.\
global: the entire model; Sheds light on the big picture biases affecting larger subgroups local: single prediction; Are predictions being made for the right reasons\
Occlusion or perturbation-based: cover a small area of the image with a black or grey square, then calculate the impact this has on the model prediction.\
Saliency maps: Highlight the area of the input that is most relevant for the model output.
:::

------------------------------------------------------------------------

## XAI Challenges I

-   Adversarial attacks have shown that:

    -   XAI methods can remain the same even when the prediction changes [@Gu2019NeuralNM]

    -   saliency maps can change without the prediction changing [@ghorbani2019interpretation]

-   Post-hoc explanations can be easily manipulated e.g. [@dombrowski2019explanations], as well as can be sensetive to hyperparameters [@bansal2020sam].

-   How can we evaluate XAI?

    -   Its subjective nature means there is no groundtruth.
    -   If we knew what was important to the model then we would not need an explanation.
    -   Just like the model can be wrong, so can the explanation.

::: notes
Adversarial attacks: Deliberate modifications to input data to deceive machine learning models, that are not noticable to humans. technique limitations
:::

------------------------------------------------------------------------

## XAI Challenges II

-   A key challenge is that terms such as interpretable, explainable, transparent, etc. have no consensus. Specifically, some authors will use these terms interchangeably [@molnar2022; @lipton2018mythos], where other authors will use different meanings for these [@Rudin2018StopEB].
    -   Even when authors explicitly state the definition they are referring to, this is still an issue.

    -   To illustrate, for the rest of this presentation when I talk about a sandwich, I am referring to a hotdog.
-   Similarly, the general purpose of XAI is to help human users better understand non human features. Many limitations have occurred from not considering the human element. Such as
    -   humans tend to give positive interpretation; we assume that the feature we would find important is the one that was used (confirmation bias).

::: notes
human focused
:::

------------------------------------------------------------------------

## XAI Challenges III

-   Despite the attention that XAI has received, there still remains a signifcant limitation in the literature; user studies.
    -   @adadi2018peeking reported that only 5% of XAI publications are user studies.

    -   While this is low, it has been estimated to be closer to 1% [@keane2019case], with many of the available user studies on XAI not reporting statistical analysis.
-   User studies for this task are not an easy problem.
    -   A simplified task is required [@lipton2018mythos].
    -   Does subjective understanding of a model reflect a participants true understanding?

::: notes
User Studies
:::

------------------------------------------------------------------------

### User Study

-   A user study was ran on Prolific [^1] to explore if post-hoc explanations help humans recognise when a face recognition model is biased.
-   The Diverse Human Faces dataset by Synthesis AI [@diversehumanfacesdataset] was used over real faces to ensure labels were correct for features such as skin tone, yaw direction, lighting, among other things.
-   Synthetic data in face recognition models has been explored
    -   Only using synthetic data can compete with some benchmarks [@wood2021fake].
    -   Synthetic data can be used to balance out a dataset of real faces when there is not a balance of the different groups, such as direction, lighting, background, etc. [@kortylewski2019analyzing].

[^1]: https://www.prolific.co/

::: notes
Note about labels in computer vision and impact it can have?
:::

------------------------------------------------------------------------

### Yaw Position

-   Categories were created based on the yaw direction.

```{r}
yaw_labels <- data.frame(
  Label = c('Middle', 'Side', 'Vacant'), 
  `Yaw Range(s)` = c('-4˚ to 4˚', '-15˚ to -7˚, 7˚ to 15˚', '-7˚ to -4˚, 4˚ to 7˚'), 
  n = c(2650, 5334, 2016), 
  Prop = c(0.265, 0.533, 0.202)
)

yaw_labels |>
  gt() |>
  tab_source_note(source_note = "Note: Side and vacant yaw ranges include two different ranges.") |>
  cols_label(Yaw.Range.s. = 'Yaw Range(s)') |>
  tab_header(title = 'Yaw Direction Labels') |>
  cols_width(Yaw.Range.s. ~ px(150))
```

::: {style="display: flex; justify-content: center;"}
<figure>

<img src="images/6899.cam_default.f_1.rgb.png" alt="Middle Yaw" style="width: 70%;"/>

<figcaption>Middle Yaw</figcaption>

</figure>

<figure>

<img src="images/6805.cam_default.f_1.rgb.png" alt="Side Yaw" style="width: 70%;"/>

<figcaption>Side Yaw</figcaption>

</figure>
:::

::: notes
Note about the influence yaw has, e.g. CV recognises looking to the side as a different item.
:::

------------------------------------------------------------------------

### Skin Tone

-   Similarly, to categorise skin tone, the Fitzpatrick classification system [@fitzpatrick1988validity] was used, where the middle group was removed to make it slightly more clear.

```{r}
skin_tone_labels <- data.frame(
  Label = c('Light Skin Tones', 'Dark Skin Tones', 'Misc'),
  `Rounded Values` = c('1, 2', '4, 5, 6', '3'), 
  n = c(3900, 2900, 3200),
  Prop = c(0.39, 0.29, 0.32)
)

skin_tone_labels |>
  gt() |>
  cols_label(Rounded.Values = 'Rounded Values') |>
  tab_header(title = 'Skin Tone Labels') |>
  cols_width(
    n ~ px(50),
    Prop ~ px(50),
    everything() ~ px(150)) |>
  cols_align(columns = c(Rounded.Values, n, Prop), align = 'left')
```

::: {style="display: flex; justify-content: center;"}
<figure>

<img src="images/input_human_6814.png" alt="Light Skin Tone" style="width: 70%;"/>

<figcaption>Light Skin Tone</figcaption>

</figure>

<figure>

<img src="images/input_human_9277.png" alt="Dark Skin Tone" style="width: 70%;"/>

<figcaption>Dark Skin Tone</figcaption>

</figure>
:::

------------------------------------------------------------------------

### Creating Groups

-   Then using *skin tone* and *yaw position*, three models were trained using a different proportion of labels.
-   For all three datasets/models there was the same proportion of sex and skin tone, what differed was the direction the human was looking at.

```{r}
data.frame(
  Model = c('Fair', 'Dark Undersampled', 'Light Undersampled'),
  yaw_dark = c('All', 'Middle', 'All'),
  yaw_light = c('All', 'All', 'Middle')
) |>
  gt() |>
  tab_spanner(label = 'Yaw Position', columns = c(yaw_dark, yaw_light)) |>
  cols_label(yaw_dark = 'Dark Skin Tones', yaw_light = 'Light Skin Tones') |>
  tab_header(title = 'Experiment Models') |>
  cols_width(
    Model ~ px(170),
    starts_with('yaw') ~ px(150)
  )
```

::: notes
Images were resizes from 1024 x 1024 to 256 x 256, cropping and architecture?
:::

------------------------------------------------------------------------

### Experiment Conditions

-   The user was provided with an **input image**, the **predicted person**, and an explanation from two difference models, asking the user to select which XAI they found to be more reasonable.
-   Each participant experienced the condition made up of the undersampled groups, with the second condition experienced being randomly assigned.
-   The order of the trials within a block was randomised, and the position of the model was counterbalanced.
-   Likewise, the order of the blocks was counterbalanced.

```{r}
exp_cond <- data.frame(
  #Block = c('A', 'B', 'C'),
  condition1 = c('Light', 'Fair', 'Fair'),
  condition2 = c('Dark', 'Dark', 'Light'),
  factor_type = c('Between-subjects', 'Within-subjects', 'Within-subjects'),
  condition_name = c('Biased', 'Fair-Dark', 'Fair-Light')
)

exp_cond |>
  gt() |>
  tab_header(title = 'Experiment Conditions') |>
  tab_footnote(footnote = 'Note: each block is counterbalanced.') |>
  tab_footnote(footnote = 'Light and Dark refer to the undersampled group.', locations = cells_column_labels(columns = c(condition1, condition2))) |>
  cols_label(condition1 = 'Condition 1', condition2 = 'Condition 2', factor_type = 'Factor Type', condition_name = 'Condition Name') |>
  cols_width(
    starts_with('condition') ~ px(125),
    factor_type ~ px(175),
    condition_name ~ px(180)
    )
```

::: notes
e.g. for the model that everybody experiences, half will see the Dark Undersampled model as **Model 1**, any the other half will see it as **Model 2**\
e.g. which condition they experience first
:::

------------------------------------------------------------------------

### Participant Info {.scrollable}

-   Sample size: Experiment (n = 40) \| Pilot (n = 10)
-   Requirements included
    -   at least 18 years old age;
    -   being from either Australia, United States, or United Kingdom;
    -   fluent in English.
-   The breakdown of countries for the experiment consisted of:
    -   Australia 7
    -   United Kingdom 23
    -   United States 10

```{r}
part_demo <- data.frame(
  age_years = c('18 - 30', '30 - 45', '45 - 65', '65 +', 'All'),
  female = c('9 (39.1)', '7 (63.6)', '0 (0.0)', '1 (50.0)', '17 (42.5)'),
  male = c('14 (60.9)', '4 (36.4)', '4 (100.0)', '1 (50.0)', '23 (57.5)'),
  all = c('23 (100.0)', '11 (100.0)', '4 (100.0)', '2 (100.0)', '40 (100.0)')
)

part_demo |>
  gt() |>
  tab_spanner(label = 'Sex, n (%)', columns = c(female, male, all)) |>
  cols_label(age_years = 'Age (Years)', female = 'Female', male = 'Male', all = 'All') |>
  tab_header(title = 'Participant Demographics') |>
  cols_width(everything() ~ px(105))
```

------------------------------------------------------------------------

### Example Trial

-   explain how XAI works using the example image as an example
-   explain the task

![Example Trial](images/xai_study_example.png){width="99%"}

::: notes
Explain how XAI works using this example.\
Explain the task.
:::

------------------------------------------------------------------------

### Selected Model

-   The Dark Undersampled model was the most selected model as the most reasonable.
-   Likewise, the balanced model was selected the least frequently as the most reasonable model

```{r}
selected_model <- data.frame(
  stat = c('Observed', 'Expected', 'Std. Residuals'),
  dark_und = c('481 (0.391)', '410 (0.33)', '4.29'),
  light_und = c('405 (0.329)', '410 (0.33)', '- .302'),
  fair = c('344 (0.280)', '410 (0.33)', '- 3.99')
)

selected_model |>
  gt(rowname_col = 'stat') |>
  cols_label(dark_und = 'Dark Undersampled', light_und = 'Light Undersampled', fair = 'Fair') |>
  cols_align(align = 'left') |>
  tab_header(title = 'Contingency Table of Selected Model') |>
  tab_footnote(footnote = 'Note: Count, (%)') |>
  cols_width(
    ends_with('_und') ~ px(160),
    fair ~ px(125)
  )
```

------------------------------------------------------------------------

### Selected Model by Confidence

::: {style="display: flex; justify-content: center;"}
![](images/selected_model_by_confidence.png){alt="Selected Model by Confidence" fig-align="center" width="70%"}
:::

::: notes
Dark undersampled was selected most, yet confidence of 1 and 2 were higher, and 3 and 4 were lower than the other two
:::

------------------------------------------------------------------------

## Strengths/Weakness of Study

-   rename this title

------------------------------------------------------------------------

## Evaluating the Results

-   hi

------------------------------------------------------------------------

## Conclusion

-   main points
-   contributions/signifcance of study
-   future research directions
-   visualise sandwich, then ask what sauce they saw on their hotdog?

------------------------------------------------------------------------

## References
